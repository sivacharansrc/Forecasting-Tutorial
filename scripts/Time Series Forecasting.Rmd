---
title: "Time Series Forecasting"
author: "Siva Chandrasekar"
date: "November 19, 2017"
output: html_document
---

```{r Loading Required Libraries, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
#install.packages(c("dplyr","fpp","plotly","dygraphs"))
rm(list = ls())
options(scipen = 999)
library(dplyr)
library(fpp)
library(plotly) #install.packages(c(fpp,dygraphs))
library(dygraphs)
wd <- "C:/Users/SSOMA/OneDrive - Monsanto/My R Projects/Machine Learning Projects/Time Series Forecasting/"

wd <- "C:/Users/sivac/Documents/R Projects/Forecasting-Tutorial/"


```


# The Forecaster's Toolbox

This repository for Forecasting Tutorial using Rob Hyndmann's source

This project has all the codes that will be described in Rob Hyndmann's tutorial available at https://www.otexts.org/fpp/2/4

QUESTIONS TO BE ASKED BEFORE FORECASTING:

- Are there consistent patterns?
- Is there a significant trend?
- Is seasonality important?
- Are there any outliers in the data that needs expertise's knowledge?

CHOOSING AND FITTING MODELS:

- Always run different models to predict the forecast, and choose a best model out of the predicted models. Severl tests are available to measure the error's, minimize the errors that will be discussed eventually.

```{r Reading Sample Time Series dataset, results='hide'}

data <- read.csv(paste0(wd,"dataSet/SampleData.csv"), header = T)
tsdata <- data$tsValue
tsdata <- ts(tsdata, start = c(2010,9), end = c(2017,8), frequency = 12)

#head(tsdata)


```

## GRAPHICS:

```{r A simple sample plot}
load(paste0(wd,"dataSet/melsyd.rda"))

plot(melsyd[,"Economy.Class"], 
     main="Economy class passengers: Melbourne-Sydney",
     xlab="Year",ylab="Thousands")



```

Looking at the plot for the Economy class, the following are evident at a first glance:

- No passengers have travelled during part of the period in 1989 - This is due to some industrial dispute (SME input)

- A period of reduced load in 1992

- A large increase in passenge load in end of 1991

- There are some large dips at the start of each year (holiday season)

- Long term flucutations: increasing in 1987, then decreasing in 1989, then again increasing through 1991, 1992

All the above observations should be accounted when building a model

```{r Another Sample Plot}

dygraph(tsdata, main = "Total Sales", ylab = "Sales in Mn") %>% 
  dyRangeSelector() %>%
  dyAxis("x", label = "Period") %>%
  dyAxis("y", axisLabelFontSize = 10) %>%
  dySeries(label = "Total Sales in USD") %>%
  dyOptions(drawPoints=T, pointSize=3, maxNumberWidth = 20)

```


A simpler time series data is available below:

```{r Plot of a Simpler Time Series data} 

load(paste0(wd,"dataSet/a10.rda"))

plot(a10,
ylab="$ million",
xlab="Year", 
main="Antidiabetic drug sales")

```

The above graph of Antibiotic drug sales shows the following characteristics:

- Clear increasing trend

- seasonal pattern exists that increases as the level of the series increases

- A dip at the end of each year (due to govt subsidization as provided by SME)

Any model used to predict the antibiotic sales should take in to account on the seasonality, and the slow but steady raising trend.

### SEASONAL PLOT AND MONTH PLOT

These plots are mainly used to visualize the data by season or a cycle which provides better idea to the seasonality behind the data

```{R Seasonplot and monthplot using sample dataset}

#load("~/R Projects/Forecasting-Tutorial/dataSet/a10.rda")

seasonplot(a10,
ylab="$ million",
xlab="Year", 
main="Antidiabetic drug sales")

seasonplot(tsdata,
     main = "Sample TS Data",
     xlab="Year",
     ylab="Sample Value",
     year.labels=TRUE, 
     year.labels.left=TRUE,
     col = 1:8,
     pch=8)

monthplot(a10,ylab="$ million",xlab="Month",xaxt="n",
          main="Seasonal deviation plot: antidiabetic drug sales")
axis(1,at=1:12,labels=month.abb,cex=0.8)

monthplot(tsdata,
          main = "Sample TS Data",
     xlab="Year",
     ylab="Sample Value")

```

### SCATTER PLOT

Scatter plots are useful to find the **relations betweek variables**. These plots can also be useful to find the strength of the relationship between a predictor variable and predicted variable.

```{r Scatter Plot Sample}

plot(fuel[,5], fuel[,8], xlab="City mpg", ylab="Carbon footprint")

```

In a scatterplot, *jitter function* is useful to avoid any overlapping of the data points.

```{r Scatter Plot using Jitter function}

plot(jitter(fuel[,5]), jitter(fuel[,8]), xlab="City mpg", ylab="Carbon footprint")

```

**Scatter Plot Matrices** are useful when there are more than two variables for which the relationship is to be determined. 

```{r Pairs plot for comparing multiple continuous variable relationship}

pairs(fuel[,-c(1:2,4,7)], pch=10)

```


Note that scatter plots should be used with *continuous variables* for finding the relationship. Ordinal or factors do not provide much information.

## NUMERICAL DATA SUMMARIES

**Percentiles** are very useful for describing the distribution of the data. 

```{r Finding distribution of data using percentiles}

quantile(fuel$Carbon, probs = .1)
fuel$Carbon
IQR(fuel$Carbon)

quantile(fuel$Carbon, probs = .75) - quantile(fuel$Carbon, probs = .25)

```


**Standard Deviation** is also used to find the spread or the distribution of the data.

```{r Standard Deviation}

sd(fuel$Carbon)

```

### BIVARIATE STATISTICS

Most common bivariate statistics is *correlation coefficient*. This is useful to measure the relationship between two variables. The expression can be explained as below:

 $$r =\frac{\sum_ ( X_i - \bar{X} ) (Y_i - \bar{Y})}{\sqrt{\sum_ ( X_i - \bar{X} )^2} \sqrt{\sum_ ( Y_i - \bar{Y} )^2}}$$

The value of correlation coefficient lies between -1 and +1, the positive values indicating linear relationship, whereas the negative values represent non-linear relationship.

### AUTO CORRELATION FACTOR

**Auto Correlation Factor (ACF)** are used in Time Series for finding the relation between lagged values of time series. For instance, $r_1$  measures the relation between time $y_t$ and $y_{t_-1}$, and $r_2$ measures the relationship between $y_t$ and $y_{t-2}$

```{r Auto Correlation Factor}

lag.plot(tsdata, lags = 12, pch=19, do.lines = F, col = 3)

```

*ACF* can be written as:

$$r_k = \frac{\sum_{t=K+1}^T (y_t - \bar{y}) (y_{t-k} - \bar{y})}{\sum_{t=1}^T (y_t - \bar{y})^2} $$

The ACF between different lags are normally plotted to form the autocorrelation graphs or the correlogram. A sample of ACF is given below:

```{r ACF Plots}

beer2 <- window(ausbeer, start=1992, end=2006-.1)

plot(beer2)

acf(beer2)

acf(tsdata)

```
     
In the beer plot, we can see that there are spikes for every 4 lags, and troughs for every two lags before the spikes. This can also mean that the data has some kind of seasonality every four months

### WHITE NOISE

A scenario where a time series data shows no correlation. 

```{r White Noise Plots}
set.seed(30)
x <- ts(rnorm(50))
plot(x, main="White noise")
acf(x)
```


In white noise, the expectation is that the ACF is close to zero. In other words, 95% of the ACF lies within $\pm \frac{2}{\sqrt{T}}$ where T is the length of time series

## SIMPLE FORECASTING METHODS

Some forecasting methods are very simple but surprisingly effective. There are four simple forecasting methods that can be used for benchmarking purpose.

### AVERAGE METHOD:

Simply done by taking the forecast of the historical values. 

$$\hat{y}_{T+h|T} = \bar{y} = (y_1 + ... + y_T) / T$$

The average method is not only applicable to timeseries data, but also applicable for other cross sectional data. All the other methods are applicable only to Time Series data sets.

```{r Average Method for Forecasting}
meanf(tsdata, 12)

```

### NAIVE METHOD

This method works on the pricinple that all future values are based on the last observed value ($y_T$). This works remarkable well for economic and financial timeseries.

```{r Naive Method}

naive(tsdata, 12)

rwf(tsdata, 12) #Alternate way

```

### SEASONAL NAIVE:

Forecast is said to be the last observed value from the previous season. This method works very good for seasonal data. 

$$y_{t+h-km} \hspace{0.2cm} where\hspace{0.2cm}m=seasonal\hspace{0.2cm}period,\hspace{0.2cm}k={(h-1)/T}+1$$
where $k={(h-1)/T}$ is the integar part of the calculation. 

```{r Seasonal Naive}

snaive(tsdata, 12)

tail(tsdata, 12) #For reference. 

```

### Drift Method for forecasting

A variation of the Naive method where the forecasts are allowed to increase or decrease, based on the amount of change over time (the drift) is set to be the average change seen in the historical data

$$y_T+\frac{h}{T-1}\hspace{0.2cm}\sum_{t=2}^T\hspace{0.2cm}(y_t-y_{t-1})\hspace{0.2cm}=\hspace{0.2cm}y_T + h(\frac{y_T-y_1}{T-1})$$

This is equal to drawing a line from 1st observation to last observation, and extrapolating the line to predict the forecast.

```{r Drift Method}

rwf(tsdata,12, drift = T)

```

*Comparing all different types of forecast, and see how they perform on predicting future values*

```{r Comparison of Simple Forecasting Methods}

beer2 <- window(ausbeer,start=1992,end=2006-.1)
beerfit1 <- meanf(beer2, h=11)
beerfit2 <- naive(beer2, h=11)
beerfit3 <- snaive(beer2, h=11)
beerfit4 <- rwf(beer2,h=11,drift=T)

plot(beerfit1, 
  main="Forecasts for quarterly beer production", col=4)
lines(beerfit2$mean,col=2)
lines(beerfit3$mean,col=3)
lines(beerfit4$mean, col=6)
legend("bottomright",lty=1,col=c(4,2,3,6),
  legend=c("Mean method","Naive method","Seasonal naive method", "Drift Method"))

### COMPARISON WITH SAMPLE TS DATA

ts2 <- window(tsdata,start=c(2010,9),end=c(2016,8))
tsfit1 <- meanf(ts2, h=11)
tsfit2 <- naive(ts2, h=11)
tsfit3 <- snaive(ts2, h=11)
tsfit4 <- rwf(ts2,h=11,drift=T)

plot(tsfit1, 
  main="Forecasts for quarterly beer production", col=4)
lines(tsfit2$mean,col=2)
lines(tsfit3$mean,col=3)
lines(tsfit4$mean, col=6)
legend("bottomleft",lty=1,col=c(4,2,3,6),
  legend=c("Mean method","Naive method","Seasonal naive method", "Drift Method"))

```
## TRANSFORMATIONS AND ADJUSTMENTS:

**Transformations** are performed over time series data in order to simplify the pattern in the historical data by removing known variations or by making the patterns more consistent across the data. Four types of transformations exist: mathematical transformations, calendar adjustments, population adjustments, and inflation adjustments.

### MATHEMATICAL TRANSFORMATIONS

These are performed on time series which show an increasing or decreasing level. One such example of mathematical transformation is the lograthmic scale. If $y_t$ represents the time series data, then $w_t$ = $log(y_t)$.

These transformations help to reduce the extensive variations in the datasets. Other similar transformations can be square root or cube root. 

The **Box-Cox transformation** is a commonly used transformation that includes both log and power transformation. They depend on the parameter $\lambda$.

```{r Box Cox Transformation Example}
plot(elec, ylab="Un-Transformed electricity demand",
 xlab="Year", main="Un-Transformed monthly electricity demand")

plot(log(elec), ylab="Transformed electricity demand",
 xlab="Year", main="Transformed monthly electricity demand")

# The BoxCox.lambda() function will choose a value of lambda for you.
lambda <- BoxCox.lambda(elec) # = 0.27
plot(BoxCox(elec,lambda))

plot(tsdata,main="TS Plot", xlab="Year", ylab="TS Value")

# BOX COX FOR TSDATA

lambda <- BoxCox.lambda(tsdata)
plot(BoxCox(tsdata, lambda), main="Box-Cox Tranformation Plot", xlab="Year", ylab="TS Value")


```


In the above graphs, the season variation is not clearly visible in the actual dataset. However, the plots with the log, and box cox transformation show a clear pattern on seasonality throughtout the series. When the seasonlaity become similar across the trend, it becomes easier for the time series models to pick them when forecasting.

However, looking at the tsdata plot, a transformation may not be necessary.

### CALENDAR ADJUSTMENTS:

These adjustments are done to elimate the differences due to the calendar period. In the below scenario, there can be some differences on the monthly milk production due to variying month periods across the years. To eliminate this effect, when we calculate the average production of milk per day, the trend and seasonality is much more smoother

```{r Calendar Adjustment Example Code}
data(milk)

monthdays <- rep(c(31,28,31,30,31,30,31,31,30,31,30,31),14)
 monthdays[26 + (4*12)*(0:2)] <- 29
par(mfrow=c(2,1))
plot(milk, main="Monthly milk production per cow",
   ylab="Pounds",xlab="Years")
plot(milk/monthdays, main="Average milk production per cow per day", 
   ylab="Pounds", xlab="Years")

```

### POPULATION ADJUSTMENTS

In any data where population is related, it is best to take considerations on per capita basis rather than the total number. For instance, when studying the number of hospital beds in a region over time, it is always better to consider the number of beds available per 100 or 1000 individuals rather than total number of beds. This is to eliminate the population error: Consider that the beds are increasing in a region, however when the population increases much faster than the beds, though the number of beds increased as a whole, they are not enough for the growing population. Studying by per capita basis will eliminate this issue.

### INFLATION ADJUSTMENTS

A $200K house worth today is not the same 20 years ago. This is due to inflation changes. To make inflation adjustments, price index is used which are supplied usually by a government agency. If $z_t$ denotes the price index, and $y_t$ denotes the price of the house at time $t$, then $x_t$ = $y_t$/$z_t$*$z_2000$ gives the adjusted price of the house at year 2000 dollar values. For consumer goods, a common price index is Consumer Price Index (or CPI)

## Evaluating Forecast Accuracy:

###### Scale Dependent Errors:

The forecast error is simply $e_i$ = $y_i$ - $\hat{y_i}$ which is on the same scale on the data. Such errors are scale dependent, and cannot be used to compare between series  that are on different scales.

The two most commonly used scale dependent measures are:

1. Mean Absolute Error: $MAE$ = $mean(|e_i|)$
2. Root Mean Square Error: $RMSE$ = $\sqrt{mean(|e_i^{2}|)}$

###### Percentage Errors:

Percentage Errors are scale independent, and so are frequently used to measure forecast performance between different datasets.$p_i = 100 * e_i/y_i$

Mean Absolute Percentage Error: $MAPE = mean(|p_i|)$

MAPE cannot be used when there are valid cases where the Actuals can be zero. For instance, the sales for a particular product may be seasonal and there are chances that the sales of the product is zero off season. Or, when forecasting temperature there are possibilities that the actual temperature can be zero. On such cases, the MAPE cannot be used as it may be equal to $\infty$ 

```{r Measuring accuracy of the forecasts}

beer3 <- window(ausbeer, start=2006)
accuracy(beerfit1, beer3)
accuracy(beerfit2, beer3)
accuracy(beerfit3, beer3)
accuracy(beerfit4, beer3)

```

Thus from the above, it is obvious that the snaive method has the best forecast accuracy among the rest.

###### Residual Diagnostics:

The difference between the predicted value and the actual value is called the residual. $Residual \space (e_i) = y_i - \hat{y_i}$.

A good forecating method will have the following characteristics of residuals:
- The residuals are uncorrelated. If they are correlated, then there is some information in the residuals that should be used in determining forecast

- Mean of residual is zero. If not, then the forecast is biased.

- Residuals have constant variance

- Residuals are normally distributed


Fixing a forecast with a residual having a mean is easy. The forecast is adjusted by adding mean to all the predicted values.

```{r Residuals of dow jones data}

dj2 <- window(dj, end=250)
par(mfrow = c(2,2))
plot(dj2, main="Dow Jones Index (daily ending 15 Jul 94)", ylab="", xlab="Day")
res <- residuals(naive(dj2))
plot(res, main="Residuals from naive method", ylab="", xlab="Day")
Acf(res, main="ACF of residuals")
hist(res, nclass="FD", main="Histogram of residuals")


```

From the graphs, it looks like there are no significant correlation in the residuals, which means all correlations are addressed in the forecast. 

Some additional tests are available for testing the correlation:

Portmanteau test for correlation or the Box-Pierce test, and Ljung-Box test are used for checking if there are any correlations in the residuals, and if they are, are they due to sampling variation? A larger p-value supports the Null Hypothesis (i.e. any correlation existing are due to sampling variation, and that the population may be devoid of any correlation), whereas a smaller value rejects the Null Hypothesis, and suggests that the forecast can be improved by utilizing the correlation that are available within the residuals.

```{r Box-Pierce Test & Ljung-Box Test for Correlation}

# lag=h: h=10 for non seasonal dataset, or h = 2m for seasonal dataset, where m is the seasonal period
# fitdf = K: K is the number of parameters. Mostly for ts, K= 0 
Box.test(res, lag=10, fitdf=0) # Box-Pierce Test

Box.test(res,lag=10, fitdf=0, type="Lj") # Ljung-Box Test

```

###### Prediction Intervals:

The range to which a prediction can vary is called the prediction Interval. We can have a 95% prediction interval, or 80% interval as required. If we are forecasting one step ahead, the sd of forecast is always almost equal to the sd of residuals, when there are no parameters involved in forecasting (in case of naive). When parameters do exist, the sd of forecast tend to be higher than residuals, but is very minimal difference that it is ignored.

Prediction Interval at 95%: $\hat{y_i}\space \pm \space 1.96\hat{\sigma}$
Prediction Interval at 80%: $\hat{y_i}\space \pm \space 1.28\hat{\sigma}$

where $\hat{y_i}$ is the estimated forecast, and $\hat{\sigma}$ is the estimate of sd of the forecast distribution. The multiplier 1.96 or 1.28 are for 95% and 80% respectively.For different percentage of prediction intervals, different multipliers to be used. (Additional information on the multiplier number is available here: $\href{https://www.otexts.org/fpp/2/7}{OTEXT LINK}$)

## SIMPLE REGRESSION

#### Simple Linear Model: 

In this model, the forecast and predictor variables are assumed to be related by a simple linear model:

$y\space=\space\beta_0+\beta_1x+e$

Here, the parameter $\beta_0$ is the predicted value of y, when x = 0, and $\beta_1$ is the predicted increase of y, when x increases by 1 unit. In a linear model, the regression line is reprented by a straight line. However, in reality not all the observed values lie in a straight line. The distance between the predicted value along the regression line, and the observed value is the error or the residual. In other words, $\beta_0+\beta_1x$ captures the explanatory part or the systematic part, whereas $e$ captures the deviation from prediction or the random error.

In a good model, we assume that:
- the errors sum up to zero, or the mean is equal to zero, else the forecast will be systematically biased
- are not correlated, else there is information available in the errors that can be used in forecast
- are unrelated to the predictor variable, else there is more information that is available in the error that should be used in the forecast

####Least Square Estimation:
The optimal values for $\beta_0$ and $\beta_1$ can be determined by minimizing the sum of the squares of the errors.  

$$\sum_{i=1}^N {e_i}^2 = \sum_{i=1}^N (y_i-\beta_0-\beta_1x_i)^2$$

#### Regression and Correlation:

Correlation and regression are closely associated with each other. The stronger the correlation exists between x and y, the more closer are the observed values lie along the regression line. The parameter $\beta_1$ can also be expressed as $$\hat\beta_1 = r\frac{S_y}{S_x} $$

where, $S_y$ and $S_x$ are respective standard deviations.

Here is a simple way to draw a regression line:

```{r Generating regression coefficients for cars data}

plot(Carbon ~ City,xlab="City (mpg)",
  ylab="Carbon footprint (tons per year)",data=fuel, col="dark green", lwd =3)
fit <- lm(Carbon ~ City, data=fuel)
abline(fit)

summary(fit)


```

**Interpretting the Regression Results:**

From the above regresion summary, $\beta_0$ is 12.53 (which says the carbon foot print would be 12.53 tonnes per year, provided x is zero), and the carbon foot print decreases by 0.22 tones per year for increase in 1 mpg (which is $\beta_0$)

####Evaluating Regression Model:

For a good linear regression model, the predictor/regressor/independent variable and the dependent variable should be in linear relation, and the residuals should have a constant variance (i.e. homocedastic nature), and exhibit no to little correlation with the predictor variables.

```{r Evaluating Regression Model}
res <- residuals(fit)
plot(res~City, main="Residuals Plot", data = fuel, col = "red", lwd = 3)
abline(0,0)
hist(res)

```

From the above residual plot, it is obvious that they exhibit some kind of relation with the predictor variable, and is not homoscedastic, as it looks like some pattern exists within the plot. For instance, most of the lower and upper values of mpg have positive values, and the values between 20 and 30 mpg fall below 0. This suggests that a simple linear model is not not appropriate, and a non linear model is required.

An outlier that has an impact on the regression line is called influential observation. Though such outliers may have an impact on the regression line, it does not always leave a large residual. For clear understanding, refer $\href{https://www.otexts.org/fpp/2/7}{Influential\space Observation}$

**Goodness of fit:** In linear regressions, the goodness of a fit is measured with the help of $R^2$ which is nothing but the square of the correlation between the predicted values and the actual observations.


$$R^2 = \frac{\sum(\hat y_i - \bar y)^2}{\sum(y_i - \bar y)^2}$$
If the predictions are close to observed values, then the value of $R^2$ will be close to 1, else close to 0. $R^2$ lies between 1 and 0. In simple linear regression, the value of $R^2$ is also equal to the square of the correlation between y and x. 


**Standard Error of the Regression**

This is one other measure to estimate the goodness of the model. This measure is obtained by calculating the standard deviation of the residuals.

$$s_e = \sqrt{\frac{1}{N-2} \sum^N_{i=1} e^2_i} $$

We have N-2, because there are two parameters involved in the linear regression. (i.e. the intercept and the slope). We can compare this error to the sample mean of yy or with the standard deviation of yy to gain some perspective on the accuracy of the model. 

#### Forecasting With Regression:

The $\hat y$ obtatined by above are not true forecasts, as the x values are already observed values. The true forecast are generated when the prediction is carried out for unknown values of x, using the generated model.

Assuming that the residuals are normally distributed, an approximate 95% forecast interval is given by: 

$\hat y \space \pm \space 1.96 s_e \sqrt {1+ \frac{1}{N} + \frac{(x- \bar x)^2}{(N-1)s_x^2}}$,

where N = Total Number of Observations
$\bar x$ is the mean of the observed values
$s_x$ is the SD of the observed values

Similarly, an 80% forecast interval can be obtained by replacing 1.96 by 1.28. The above equation shows that the forecast interval becomes wider when the value of x moves far away from mean of x. i.e. we are more certain on the forecast when the predictor variable is close to the sample mean.

The estimated regression line in the Car data example is $\hat y = 12.53−0.22x$.

For the Chevrolet Aveo (the first car in the list) $x_1$ = 25 mpg and $y_1$ =6.6 tons of CO2 per year. The model returns a fitted value of $\hat y_1$ = 7.00, i.e. $e_1$ = −0.4. For a car with City driving fuel economy $x$ =30 mpg, the average footprint forecasted is $\hat y$ = 5.90 tons of CO2 per year. The corresponding 95% and 80% forecast intervals are (4.95,6.84) and (5.28,6.51) respectively (calculated using R)

```{r Forecast with 80% and 95% Prediction Intervals}
#fitted(fit)[1]
#fcast <- forecast(fit, newdata=data.frame(City=30))
#plot(fcast, xlab="City (mpg)", ylab="Carbon footprint (tons per #year)")
# The displayed graph uses jittering, while the code above does not.


```


#### Statistical Inference:


